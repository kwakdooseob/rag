# -*- coding: utf-8 -*-
from dotenv import load_dotenv
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
import sys
import time
import shutil
import requests
from bs4 import BeautifulSoup
import json
import re

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException, StaleElementReferenceException

load_dotenv()

# --- Environment Variables ---
OLLAMA_HOST = os.environ.get('OLLAMA_HOST', 'http://127.0.0.1:11434')
OLLAMA_EMBEDDING_MODEL = os.environ.get('OLLAMA_EMBEDDING_MODEL', 'nomic-embed-text')
CHROMA_DB_PATH = os.environ.get('CHROMA_DB_PATH', './chroma_db')

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

CHROMEDRIVER_PATH = os.path.join(os.path.dirname(__file__), 'chromedriver.exe') 

print(f"Ollama Host: {OLLAMA_HOST}")
print(f"Embedding Model: {OLLAMA_EMBEDDING_MODEL}")
print(f"Chroma DB Path: {CHROMA_DB_PATH}")
print(f"Naver Client ID (first 5 chars): {NAVER_CLIENT_ID[:5] if NAVER_CLIENT_ID else 'Not Set'}")
print(f"ChromeDriver Path: {CHROMEDRIVER_PATH}")

if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
    print("âŒ ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

NAVER_BLOG_SEARCH_API_URL = "https://openapi.naver.com/v1/search/blog.json"
NAVER_API_HEADERS = {
    "X-Naver-Client-Id": NAVER_CLIENT_ID,
    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
}

# --- Selenium WebDriver ì´ˆê¸°í™”/ì¢…ë£Œ í•¨ìˆ˜ (ê° ìŠ¤í¬ë˜í•‘ë§ˆë‹¤ í˜¸ì¶œ) ---
def get_new_driver():
    """ìƒˆë¡œìš´ Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("Initializing Chrome WebDriver for scraping...")
    options = webdriver.ChromeOptions()
    options.add_argument('headless')
    options.add_argument('window-size=1920x1080')
    options.add_argument('disable-gpu')
    options.add_argument('no-sandbox')
    options.add_argument('disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-logging"])

    try:
        service = Service(executable_path=CHROMEDRIVER_PATH)
        driver_instance = webdriver.Chrome(service=service, options=options)
        print("Chrome WebDriver initialized.")
        return driver_instance
    except WebDriverException as e:
        print(f"âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"ğŸ’¡ ChromeDriver ({CHROMEDRIVER_PATH})ê°€ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆê³ , Chrome ë¸Œë¼ìš°ì € ë²„ì „ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None

def quit_driver_safely(driver_instance):
    """WebDriver ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ë‹«ìŠµë‹ˆë‹¤."""
    if driver_instance:
        try:
            driver_instance.quit()
            print("Chrome WebDriver closed.")
        except Exception as e:
            print(f"âš ï¸ WebDriver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- ì›¹ ìŠ¤í¬ë˜í•‘ í—¬í¼ í•¨ìˆ˜ (Selenium ê¸°ë°˜) ---
def scrape_blog_content_selenium(url: str) -> str:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš©ì„ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    (ë™ì  ë¡œë”© ë° iframe ì²˜ë¦¬)
    """
    if "n.news.naver.com" in url or "tistory.com" in url or "daum.net" in url or "post.naver.com" in url:
        print(f"   â„¹ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤/íƒ€ í”Œë«í¼/ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ë§í¬ëŠ” ìŠ¤í¬ë˜í•‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {url}")
        return ""
    
    if not "blog.naver.com" in url:
        print(f"   â„¹ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë„ë©”ì¸ì´ ì•„ë‹ˆë¯€ë¡œ ìŠ¤í¬ë˜í•‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {url}")
        return ""

    browser = None
    content_text = ""

    try:
        browser = get_new_driver() # ê° ìŠ¤í¬ë˜í•‘ë§ˆë‹¤ ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        if browser is None:
            return "" # ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

        print(f"   ğŸŒ ë¸Œë¼ìš°ì €: {url} ë¡œë“œ ì¤‘...")
        browser.get(url)
        WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        time.sleep(2)

        target_html_source = browser.page_source
        current_url = browser.current_url

        try:
            iframe_ids = ['mainFrame', 'screenFrame'] 
            iframe_element = None
            for iframe_id in iframe_ids:
                try:
                    iframe_element = WebDriverWait(browser, 3).until(
                        EC.presence_of_element_located((By.ID, iframe_id))
                    )
                    if iframe_element:
                        break
                except TimeoutException:
                    continue

            if iframe_element:
                iframe_src = iframe_element.get_attribute('src')
                if iframe_src.startswith('//'):
                    iframe_url = "https:" + iframe_src
                elif iframe_src.startswith('/'):
                    iframe_url = "https://blog.naver.com" + iframe_src
                else:
                    iframe_url = iframe_src

                print(f"   ğŸ” iframe (ID: {iframe_element.get_attribute('id')}) ë°œê²¬. URL: {iframe_url} ì „í™˜ ì‹œë„ ì¤‘...")
                
                if iframe_url and "blog.naver.com" in iframe_url and iframe_url != current_url:
                    browser.switch_to.frame(iframe_element)
                    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                    time.sleep(1)
                    target_html_source = browser.page_source
                    print(f"   âœ… iframe ë‚´ë¶€ë¡œ ì „í™˜ ì„±ê³µ.")
                else:
                    print(f"   âš ï¸ iframe URLì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë©”ì¸ í˜ì´ì§€ì™€ ë™ì¼. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©: {iframe_url}")
            else:
                print(f"   âš ï¸ í˜ì´ì§€ì—ì„œ ì í•©í•œ iframeì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©.")
        except StaleElementReferenceException:
            print(f"   âš ï¸ StaleElementReferenceException ë°œìƒ. í˜ì´ì§€ ë¦¬ë¡œë“œ í›„ ì¬ì‹œë„.")
            browser.get(url)
            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
            time.sleep(2)
            target_html_source = browser.page_source
        except Exception as e:
            print(f"   âš ï¸ iframe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©.")
            target_html_source = browser.page_source

        soup_to_parse = BeautifulSoup(target_html_source, 'html.parser')

        content_selectors = [
            'div.se-main-container',
            'div.se-viewer',
            'div#postListBody',
            'div#post-view',
            'div.blog_post',
            'div.se_component_wrap',
            'div.post-content',
            'div.post_ct',
            'div.blog_content',
            'div.area_view',
            'div.section_article',
            'div[id^="post-area"]',
            'div.se_component'
        ]
        
        content_element_soup = None
        for selector in content_selectors:
            try:
                content_element_soup = soup_to_parse.select_one(selector)
                if content_element_soup:
                    print(f"   âœ… ë³¸ë¬¸ ì½˜í…ì¸  ì˜ì—­ ë°œê²¬ (CSS: {selector})")
                    break
            except Exception:
                continue
        
        if content_element_soup:
            for unwanted_tag in content_element_soup(['script', 'style', 'noscript', 'img', 'a', 'iframe', 'video', 'audio', 'header', 'footer', 'nav', 'form']):
                unwanted_tag.extract()
            
            content_text = content_element_soup.get_text(separator='\n', strip=True)
            content_text = re.sub(r'\n\s*\n', '\n\n', content_text).strip()
            
        else:
            print(f"   âš ï¸ íŠ¹ì • ë³¸ë¬¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ íƒœê·¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            possible_main_content_area = soup_to_parse.find('div', class_='se-main-area') or \
                                         soup_to_parse.find('div', id='viewTypeSelector') or \
                                         soup_to_parse.find('div', class_='post-content') or \
                                         soup_to_parse.find('div', class_='wrap_blogview') or \
                                         soup_to_parse.find('body')

            if possible_main_content_area:
                for unwanted_tag in possible_main_content_area(['script', 'style', 'noscript', 'img', 'a', 'iframe', 'video', 'audio', 'header', 'footer', 'nav', 'form']):
                    unwanted_tag.extract()
                
                paragraphs = possible_main_content_area.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'li', 'span', 'strong'])
                if paragraphs:
                    content_text = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    content_text = re.sub(r'\n\s*\n', '\n\n', content_text).strip()
                    if content_text:
                        print(f"   â„¹ï¸ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¼ë¶€ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
            
            if not content_text:
                print(f"   âŒ ë³¸ë¬¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ìµœì¢… ì‹¤íŒ¨: {url}")

        return content_text.strip()
            
    except TimeoutException:
        print(f"   âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼ ({url})")
        return ""
    except WebDriverException as e:
        print(f"   âš ï¸ WebDriver ì˜¤ë¥˜ ({url}): {e}")
        return ""
    except Exception as e:
        print(f"   âš ï¸ ë¸”ë¡œê·¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({url}): {e}. íƒ€ì…: {type(e).__name__}")
        return ""
    finally:
        # iframeì—ì„œ ì „í™˜í–ˆìœ¼ë©´ ì›ë˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ëŒì•„ê°€ì•¼ í•¨ (í˜„ì¬ëŠ” driverë¥¼ ë°”ë¡œ ë‹«ìœ¼ë¯€ë¡œ ë¶ˆí•„ìš”)
        # try:
        #     browser.switch_to.default_content() 
        # except Exception as e:
        #     print(f"   âš ï¸ ë“œë¼ì´ë²„ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        quit_driver_safely(browser) # ê° ìŠ¤í¬ë˜í•‘ í›„ì— ë“œë¼ì´ë²„ ë‹«ê¸°


def retrieve_and_split_naver_blog_contents(queries: list, num_results_per_query: int = 5):
    """
    Retrieves blog posts from Naver Search API, then scrapes their content using Selenium,
    and returns them as Document objects.
    """
    if not queries:
        print("ğŸ’¡ ê²€ìƒ‰í•  ì¿¼ë¦¬ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    print("\n--- 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ê²€ìƒ‰ ë° ë¡œë“œ ë° ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ---")
    all_documents = []
    
    for query in queries:
        query = query.strip()
        if not query: continue

        print(f"\n'{query}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘...")
        current_query_results = []
        start_index = 1 

        while len(current_query_results) < num_results_per_query:
            display_count = min(num_results_per_query - len(current_query_results), 100)
            if display_count <= 0:
                break

            params = {
                "query": query,
                "display": display_count,
                "start": start_index,
                "sort": "sim"
            }
            
            try:
                response = requests.get(NAVER_BLOG_SEARCH_API_URL, headers=NAVER_API_HEADERS, params=params, timeout=10)
                response.raise_for_status()
                search_data = json.loads(response.text)
                
                items = search_data.get('items', [])
                
                unique_items = []
                existing_links = {doc.metadata['source'] for doc in all_documents}
                for item in items:
                    clean_link = item.get('link')
                    if clean_link and not clean_link.startswith(('http://', 'https://')):
                        continue
                    if clean_link not in existing_links:
                        unique_items.append(item)
                        existing_links.add(clean_link)
                current_query_results.extend(unique_items)
                
                if not items or len(items) < display_count:
                    break
                
                start_index += display_count
                time.sleep(0.1)
            except requests.exceptions.RequestException as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ìš”ì²­ ì˜¤ë¥˜ ('{query}'): {e}")
                break
            except json.JSONDecodeError as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜ ('{query}'): {e}")
                break
            except Exception as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ('{query}'): {e}")
                break
        
        if not current_query_results:
            print(f"'{query}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        for i, res in enumerate(current_query_results):
            blog_link = res.get("link")
            
            title_clean = res.get('title', '').replace("<b>", "").replace("</b>", "").replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
            desc_clean = res.get('description', '').replace("<b>", "").replace("</b>", "").replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")

            if not blog_link or not blog_link.startswith(('http://', 'https://')):
                print(f"   âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë¸”ë¡œê·¸ ë§í¬ ê±´ë„ˆë›°ê¸°: {blog_link}")
                content = f"ì œëª©: {title_clean}\nì„¤ëª…: {desc_clean}\n(ìœ íš¨í•˜ì§€ ì•Šì€ ë§í¬)"
                doc = Document(page_content=content, metadata={"source": "invalid_link", "title": title_clean, "query": query, "pub_date": res.get("postdate", "")})
                all_documents.append(doc)
                continue 

            print(f"   ğŸ”— '{title_clean}' ({blog_link}) ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹œë„ ì¤‘...")
            full_blog_content = scrape_blog_content_selenium(blog_link) 
            
            content = f"ì œëª©: {title_clean}\nì„¤ëª…: {desc_clean}\n"
            if full_blog_content:
                content += f"ë³¸ë¬¸:\n{full_blog_content}"
            else:
                content += "(ë³¸ë¬¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ)"

            doc = Document(
                page_content=content,
                metadata={
                    "source": blog_link,
                    "title": title_clean,
                    "query": query,
                    "pub_date": res.get("postdate", "")
                }
            )
            all_documents.append(doc)
            time.sleep(1)
        print(f"'{query}'ì— ëŒ€í•´ {len(current_query_results)}ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        time.sleep(2)

    if not all_documents:
        print("â›” ê²€ìƒ‰ ë° ìŠ¤í¬ë˜í•‘ëœ ë¬¸ì„œê°€ ì—†ì–´ ì¸ë±ì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë„¤ì´ë²„ API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return []

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len,
        add_start_index=True,
    )

    chunks = text_splitter.split_documents(all_documents)
    print(f"ì´ {len(all_documents)}ê°œì˜ ê²€ìƒ‰ ë° ìŠ¤í¬ë˜í•‘ëœ ë¬¸ì„œì—ì„œ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    return chunks

def embed_and_store(chunks, db_path, embeddings_function):
    """
    Embeds fragmented text chunks and stores them in Chroma DB.
    """
    print("\n--- 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/ì—…ë°ì´íŠ¸ ---")
    try:
        if os.path.exists(db_path):
            print(f"ê¸°ì¡´ Chroma DB '{db_path}'ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
            shutil.rmtree(db_path)
            
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings_function,
            persist_directory=db_path
        )
        print(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ê°€ '{db_path}' ê²½ë¡œì˜ Chroma DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ Chroma DBì— ë¬¸ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    print("--- RAG ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ---")
    
    # ë“œë¼ì´ë²„ëŠ” ê° ìŠ¤í¬ë˜í•‘ë§ˆë‹¤ ìƒì„±/ì¢…ë£Œí•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ ì „ì—­ ë“œë¼ì´ë²„ë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”í•  í•„ìš” ì—†ìŒ

    try:
        # Load embedding model
        embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        print(f"Ollama Embedding Model '{OLLAMA_EMBEDDING_MODEL}' ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ Ollama Embedding Model ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ì§€ì •ëœ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì˜ˆ: ollama serve ì‹¤í–‰ í›„, ollama pull {OLLAMA_EMBEDDING_MODEL}")
        # close_driver() # ì´ì œ ì „ì—­ ë“œë¼ì´ë²„ê°€ ì—†ìœ¼ë¯€ë¡œ í•„ìš” ì—†ìŒ
        sys.exit(1)

    try:
        while True:
            user_input_queries = input(
                "\në„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë§›ì§‘ ì¶”ì²œ, ì œì£¼ë„ ì—¬í–‰, íŒŒì´ì¬): "
                "\në˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì—¬ ì¢…ë£Œí•˜ê³ , 'clear'ë¥¼ ì…ë ¥í•˜ì—¬ ê¸°ì¡´ DBë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: "
            )
            
            if user_input_queries.lower() == 'exit':
                break
            elif user_input_queries.lower() == 'clear':
                if os.path.exists(CHROMA_DB_PATH):
                    print(f"ê¸°ì¡´ Chroma DB '{CHROMA_DB_PATH}'ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
                    shutil.rmtree(CHROMA_DB_PATH)
                    print("Chroma DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    print("ì‚­ì œí•  Chroma DBê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            search_queries = [q.strip() for q in user_input_queries.split(',') if q.strip()]
            
            if not search_queries:
                print("âŒ ìœ íš¨í•œ ê²€ìƒ‰ì–´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                continue

            num_blog_results_per_query = int(input(f"ê° í‚¤ì›Œë“œë‹¹ ëª‡ ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ìµœëŒ€ 100ê°œ, ê¸°ë³¸ê°’: 5): ") or "5")
            if num_blog_results_per_query > 100:
                num_blog_results_per_query = 100
                print("ğŸ’¡ ë„¤ì´ë²„ ë¸”ë¡œê·¸ APIëŠ” í•œ í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 100ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 100ê°œë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            elif num_blog_results_per_query < 1:
                num_blog_results_per_query = 5
                print("ğŸ’¡ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ëŠ” ìµœì†Œ 1ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤. 5ê°œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")


            all_chunks = retrieve_and_split_naver_blog_contents(search_queries, num_blog_results_per_query) 
            
            if all_chunks:
                embed_and_store(all_chunks, CHROMA_DB_PATH, embeddings)
            else:
                print("â›” ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë„¤ì´ë²„ API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
                
    except Exception as e:
        print(f"\nğŸš¨ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # close_driver() # ì´ì œ ê° ìŠ¤í¬ë˜í•‘ë§ˆë‹¤ ë“œë¼ì´ë²„ë¥¼ ë‹«ìœ¼ë¯€ë¡œ ì´ ë¶€ë¶„ì€ í•„ìš” ì—†ìŒ
        print("--- RAG ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ---")