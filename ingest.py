# -*- coding: utf-8 -*-
from dotenv import load_dotenv
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
import sys
import time
import shutil
import requests
from bs4 import BeautifulSoup
import json
import re

# Selenium ê´€ë ¨ ì„í¬íŠ¸
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException, StaleElementReferenceException

# Langchain Document Loaders for local files
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader, CSVLoader, UnstructuredExcelLoader
# Unstructured ë¡œë”ëŠ” unstructured ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”
# from langchain_community.document_loaders import UnstructuredWordDocumentLoader, UnstructuredFileLoader 

# Load environment variables from .env file
load_dotenv()

# --- Environment Variables ---
OLLAMA_HOST = os.environ.get('OLLAMA_HOST', 'http://127.0.0.1:11434')
OLLAMA_EMBEDDING_MODEL = os.environ.get('OLLAMA_EMBEDDING_MODEL', 'nomic-embed-text')
CHROMA_DB_PATH = os.environ.get('CHROMA_DB_PATH', './chroma_db')

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

LOCAL_DOCS_PATH = os.environ.get('LOCAL_DOCS_PATH', './local_documents') # ë¡œì»¬ ë¬¸ì„œ ê²½ë¡œ ì¶”ê°€

# ChromeDriver path configuration
CHROMEDRIVER_PATH = os.path.join(os.path.dirname(__file__), 'chromedriver.exe') 

print(f"Ollama Host: {OLLAMA_HOST}")
print(f"Embedding Model: {OLLAMA_EMBEDDING_MODEL}")
print(f"Chroma DB Path: {CHROMA_DB_PATH}")
print(f"Naver Client ID (first 5 chars): {NAVER_CLIENT_ID[:5] if NAVER_CLIENT_ID else 'Not Set'}")
print(f"ChromeDriver Path: {CHROMEDRIVER_PATH}")
print(f"Local Documents Path: {LOCAL_DOCS_PATH}")

# Validate Naver API credentials
if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
    print("âŒ ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

# Naver Search API Configuration
NAVER_BLOG_SEARCH_API_URL = "https://openapi.naver.com/v1/search/blog.json"
NAVER_API_HEADERS = {
    "X-Naver-Client-Id": NAVER_CLIENT_ID,
    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
}

# --- Selenium WebDriver ì´ˆê¸°í™”/ì¢…ë£Œ í•¨ìˆ˜ ---
def get_new_driver():
    print("Initializing Chrome WebDriver for scraping...")
    options = webdriver.ChromeOptions()
    options.add_argument('headless')
    options.add_argument('window-size=1920x1080')
    options.add_argument('disable-gpu')
    options.add_argument('no-sandbox')
    options.add_argument('disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-logging"])

    try:
        service = Service(executable_path=CHROMEDRIVER_PATH)
        driver_instance = webdriver.Chrome(service=service, options=options)
        print("Chrome WebDriver initialized.")
        return driver_instance
    except WebDriverException as e:
        print(f"âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"ğŸ’¡ ChromeDriver ({CHROMEDRIVER_PATH})ê°€ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆê³ , Chrome ë¸Œë¼ìš°ì € ë²„ì „ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None

def quit_driver_safely(driver_instance):
    if driver_instance:
        try:
            driver_instance.quit()
            print("Chrome WebDriver closed.")
        except Exception as e:
            print(f"âš ï¸ WebDriver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- ì›¹ ìŠ¤í¬ë˜í•‘ í—¬í¼ í•¨ìˆ˜ (Selenium ê¸°ë°˜) ---
def scrape_blog_content_selenium(url: str) -> str:
    if "n.news.naver.com" in url or "tistory.com" in url or "daum.net" in url or "post.naver.com" in url:
        print(f"   â„¹ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤/íƒ€ í”Œë«í¼/ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ë§í¬ëŠ” ìŠ¤í¬ë˜í•‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {url}")
        return ""
    
    if not "blog.naver.com" in url:
        print(f"   â„¹ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë„ë©”ì¸ì´ ì•„ë‹ˆë¯€ë¡œ ìŠ¤í¬ë˜í•‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {url}")
        return ""

    browser = None
    content_text = ""

    try:
        browser = get_new_driver()
        if browser is None:
            return ""

        print(f"   ğŸŒ ë¸Œë¼ìš°ì €: {url} ë¡œë“œ ì¤‘...")
        browser.get(url)
        WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        time.sleep(2)

        target_html_source = browser.page_source
        current_url = browser.current_url

        try:
            iframe_ids = ['mainFrame', 'screenFrame'] 
            iframe_element = None
            for iframe_id in iframe_ids:
                try:
                    iframe_element = WebDriverWait(browser, 3).until(
                        EC.presence_of_element_located((By.ID, iframe_id))
                    )
                    if iframe_element:
                        break
                except TimeoutException:
                    continue

            if iframe_element:
                iframe_src = iframe_element.get_attribute('src')
                if iframe_src.startswith('//'):
                    iframe_url = "https:" + iframe_src
                elif iframe_src.startswith('/'):
                    iframe_url = "https://blog.naver.com" + iframe_src
                else:
                    iframe_url = iframe_src

                print(f"   ğŸ” iframe (ID: {iframe_element.get_attribute('id')}) ë°œê²¬. URL: {iframe_url} ì „í™˜ ì‹œë„ ì¤‘...")
                
                if iframe_url and "blog.naver.com" in iframe_url and iframe_url != current_url:
                    browser.switch_to.frame(iframe_element)
                    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                    time.sleep(1)
                    target_html_source = browser.page_source
                    print(f"   âœ… iframe ë‚´ë¶€ë¡œ ì „í™˜ ì„±ê³µ.")
                else:
                    print(f"   âš ï¸ iframe URLì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë©”ì¸ í˜ì´ì§€ì™€ ë™ì¼. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©: {iframe_url}")
            else:
                print(f"   âš ï¸ í˜ì´ì§€ì—ì„œ ì í•©í•œ iframeì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©.")
        except StaleElementReferenceException:
            print(f"   âš ï¸ StaleElementReferenceException ë°œìƒ. í˜ì´ì§€ ë¦¬ë¡œë“œ í›„ ì¬ì‹œë„.")
            browser.get(url)
            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
            time.sleep(2)
            target_html_source = browser.page_source
        except Exception as e:
            print(f"   âš ï¸ iframe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©.")
            target_html_source = browser.page_source

        soup_to_parse = BeautifulSoup(target_html_source, 'html.parser')

        content_selectors = [
            'div.se-main-container',
            'div.se-viewer',
            'div#postListBody',
            'div#post-view',
            'div.blog_post',
            'div.se_component_wrap',
            'div.post-content',
            'div.post_ct',
            'div.blog_content',
            'div.area_view',
            'div.section_article',
            'div[id^="post-area"]',
            'div.se_component'
        ]
        
        content_element_soup = None
        for selector in content_selectors:
            try:
                content_element_soup = soup_to_parse.select_one(selector)
                if content_element_soup:
                    print(f"   âœ… ë³¸ë¬¸ ì½˜í…ì¸  ì˜ì—­ ë°œê²¬ (CSS: {selector})")
                    break
            except Exception:
                continue
        
        if content_element_soup:
            for unwanted_tag in content_element_soup(['script', 'style', 'noscript', 'img', 'a', 'iframe', 'video', 'audio', 'header', 'footer', 'nav', 'form']):
                unwanted_tag.extract()
            
            content_text = content_element_soup.get_text(separator='\n', strip=True)
            content_text = re.sub(r'\n\s*\n', '\n\n', content_text).strip()
            
        else:
            print(f"   âš ï¸ íŠ¹ì • ë³¸ë¬¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ íƒœê·¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            possible_main_content_area = soup_to_parse.find('div', class_='se-main-area') or \
                                         soup_to_parse.find('div', id='viewTypeSelector') or \
                                         soup_to_parse.find('div', class_='post-content') or \
                                         soup_to_parse.find('div', class_='wrap_blogview') or \
                                         soup_to_parse.find('body')

            if possible_main_content_area:
                for unwanted_tag in possible_main_content_area(['script', 'style', 'noscript', 'img', 'a', 'iframe', 'video', 'audio', 'header', 'footer', 'nav', 'form']):
                    unwanted_tag.extract()
                
                paragraphs = possible_main_content_area.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'li', 'span', 'strong'])
                if paragraphs:
                    content_text = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    content_text = re.sub(r'\n\s*\n', '\n\n', content_text).strip()
                    if content_text:
                        print(f"   â„¹ï¸ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¼ë¶€ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
            
            if not content_text:
                print(f"   âŒ ë³¸ë¬¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ìµœì¢… ì‹¤íŒ¨: {url}")

        return content_text.strip()
            
    except TimeoutException:
        print(f"   âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼ ({url})")
        return ""
    except WebDriverException as e:
        print(f"   âš ï¸ WebDriver ì˜¤ë¥˜ ({url}): {e}")
        return ""
    except Exception as e:
        print(f"   âš ï¸ ë¸”ë¡œê·¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({url}): {e}. íƒ€ì…: {type(e).__name__}")
        return ""
    finally:
        try:
            if browser:
                browser.switch_to.default_content() 
        except Exception as e:
            print(f"   âš ï¸ ë“œë¼ì´ë²„ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        quit_driver_safely(browser)


def retrieve_and_split_naver_blog_contents(queries: list, num_results_per_query: int = 5):
    if not queries:
        print("ğŸ’¡ ê²€ìƒ‰í•  ì¿¼ë¦¬ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    print("\n--- 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ê²€ìƒ‰ ë° ë¡œë“œ ë° ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ---")
    all_documents = []
    
    for query in queries:
        query = query.strip()
        if not query: continue

        print(f"\n'{query}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘...")
        current_query_results = []
        start_index = 1 

        while len(current_query_results) < num_results_per_query:
            display_count = min(num_results_per_query - len(current_query_results), 100)
            if display_count <= 0:
                break

            params = {
                "query": query,
                "display": display_count,
                "start": start_index,
                "sort": "sim"
            }
            
            try:
                response = requests.get(NAVER_BLOG_SEARCH_API_URL, headers=NAVER_API_HEADERS, params=params, timeout=10)
                response.raise_for_status()
                search_data = json.loads(response.text)
                
                items = search_data.get('items', [])
                
                unique_items = []
                existing_links = {doc.metadata['source'] for doc in all_documents}
                for item in items:
                    clean_link = item.get('link')
                    if clean_link and not clean_link.startswith(('http://', 'https://')):
                        continue
                    if clean_link not in existing_links:
                        unique_items.append(item)
                        existing_links.add(clean_link)
                current_query_results.extend(unique_items)
                
                if not items or len(items) < display_count:
                    break
                
                start_index += display_count
                time.sleep(0.1)
            except requests.exceptions.RequestException as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ìš”ì²­ ì˜¤ë¥˜ ('{query}'): {e}")
                break
            except json.JSONDecodeError as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜ ('{query}'): {e}")
                break
            except Exception as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ('{query}'): {e}")
                break
        
        if not current_query_results:
            print(f"'{query}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        for i, res in enumerate(current_query_results):
            blog_link = res.get("link")
            
            title_clean = res.get('title', '').replace("<b>", "").replace("</b>", "").replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
            desc_clean = res.get('description', '').replace("<b>", "").replace("</b>", "").replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")

            if not blog_link or not blog_link.startswith(('http://', 'https://')):
                print(f"   âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë¸”ë¡œê·¸ ë§í¬ ê±´ë„ˆë›°ê¸°: {blog_link}")
                content = f"ì œëª©: {title_clean}\nì„¤ëª…: {desc_clean}\n(ìœ íš¨í•˜ì§€ ì•Šì€ ë§í¬)"
                doc = Document(page_content=content, metadata={"source": "invalid_link", "title": title_clean, "query": query, "pub_date": res.get("postdate", ""), "source_type": "blog"})
                all_documents.append(doc)
                continue 

            print(f"   ğŸ”— '{title_clean}' ({blog_link}) ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹œë„ ì¤‘...")
            full_blog_content = scrape_blog_content_selenium(blog_link) 
            
            content = f"ì œëª©: {title_clean}\nì„¤ëª…: {desc_clean}\n"
            if full_blog_content:
                content += f"ë³¸ë¬¸:\n{full_blog_content}"
            else:
                content += "(ë³¸ë¬¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ)"

            doc = Document(
                page_content=content,
                metadata={
                    "source": blog_link,
                    "title": title_clean,
                    "query": query,
                    "pub_date": res.get("postdate", ""),
                    "source_type": "blog" # ë©”íƒ€ë°ì´í„°ì— source_type ì¶”ê°€
                }
            )
            all_documents.append(doc)
            time.sleep(1)
        print(f"'{query}'ì— ëŒ€í•´ {len(current_query_results)}ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        time.sleep(2)

    if not all_documents:
        print("â›” ê²€ìƒ‰ ë° ìŠ¤í¬ë˜í•‘ëœ ë¬¸ì„œê°€ ì—†ì–´ ì¸ë±ì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë„¤ì´ë²„ API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return []

    return all_documents


def load_local_documents_and_split(local_docs_path: str):
    """
    Loads documents from a local directory, splits them into chunks.
    Supports .txt, .pdf, .docx, .csv, .xlsx.
    """
    print(f"\n--- ë¡œì»¬ ë¬¸ì„œ '{local_docs_path}' ë¡œë“œ ë° ë¶„í•  ---")
    
    if not os.path.exists(local_docs_path):
        print(f"âŒ ë¡œì»¬ ë¬¸ì„œ ê²½ë¡œ '{local_docs_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë”ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return []

    all_loaded_docs = []
    
    # Using DirectoryLoader to load multiple file types
    # Note: For Unstructured loaders, you need 'unstructured' installed
    # For .pdf, 'pypdf'
    # For .docx, 'python-docx'
    # For .xlsx, 'openpyxl'
    
    loader_mapping = {
        ".txt": TextLoader,
        ".pdf": PyPDFLoader,
        ".docx": Docx2txtLoader, # Docx2txtLoader is simpler than UnstructuredWordDocumentLoader
        ".csv": CSVLoader,
        # ".xlsx": UnstructuredExcelLoader # UnstructuredExcelLoader requires unstructured library
    }

    # DirectoryLoader can handle multiple extensions
    # However, it might be more robust to loop through files and load individually for better error handling per file
    
    documents = []
    for root, _, files in os.walk(local_docs_path):
        for file_name in files:
            file_path = os.path.join(root, file_name)
            file_extension = os.path.splitext(file_name)[1].lower()

            loader_class = loader_mapping.get(file_extension)
            if loader_class:
                print(f"   ğŸ“„ ë¡œì»¬ ë¬¸ì„œ ë¡œë“œ ì¤‘: {file_name} (ìœ í˜•: {file_extension})")
                try:
                    loader = loader_class(file_path)
                    loaded_docs = loader.load()
                    for doc in loaded_docs:
                        doc.metadata["source"] = file_path
                        doc.metadata["title"] = file_name
                        doc.metadata["source_type"] = "local_doc" # ë©”íƒ€ë°ì´í„°ì— source_type ì¶”ê°€
                        documents.append(doc)
                    print(f"   âœ… ë¡œì»¬ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {file_name}")
                except Exception as e:
                    print(f"   âŒ ë¡œì»¬ ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_name}): {e}")
            else:
                print(f"   âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ê±´ë„ˆë›°ê¸°: {file_name}")
    
    if not documents:
        print(f"ğŸ’¡ ë¡œì»¬ ë¬¸ì„œ í´ë” '{local_docs_path}'ì— ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    return documents # ì¼ë‹¨ Document ê°ì²´ë¥¼ ë°˜í™˜í•˜ê³ , ì™¸ë¶€ì—ì„œ text_splitterë¡œ ë¶„í• 

def embed_and_store(chunks, db_path, embeddings_function):
    print("\n--- 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/ì—…ë°ì´íŠ¸ ---")
    try:
        if os.path.exists(db_path):
            print(f"ê¸°ì¡´ Chroma DB '{db_path}'ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
            shutil.rmtree(db_path)
            
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings_function,
            persist_directory=db_path
        )
        print(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ê°€ '{db_path}' ê²½ë¡œì˜ Chroma DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ Chroma DBì— ë¬¸ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    print("--- RAG ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ---")
    
    try:
        embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        print(f"Ollama Embedding Model '{OLLAMA_EMBEDDING_MODEL}' ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ Ollama Embedding Model ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ì§€ì •ëœ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì˜ˆ: ollama serve ì‹¤í–‰ í›„, ollama pull {OLLAMA_EMBEDDING_MODEL}")
        sys.exit(1)

    try:
        final_documents_to_chunk = []

        while True:
            indexing_choice = input(
                "\nì–´ë–¤ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ ì¸ë±ì‹±í• ê¹Œìš”? (1: ë¸”ë¡œê·¸, 2: ë¡œì»¬ ë¬¸ì„œ, 3: ë‘˜ ë‹¤, exit: ì¢…ë£Œ): "
            ).strip().lower()

            if indexing_choice == 'exit':
                break
            elif indexing_choice == '1' or indexing_choice == 'ë¸”ë¡œê·¸':
                search_queries_list = [
                    "ë³´í—˜ìƒí’ˆ ë¹„êµ",
                    "ë³´ì¥ ë¶„ì„",
                    "ë³´í—˜ ë¦¬ëª¨ë¸ë§",
                    "ìƒëª…ë³´í—˜ ì„¤ê³„ (ì¢…ì‹ /ì •ê¸°/ë³€ì•¡ ë“±)",
                    "ì‹¤ë¹„ë³´í—˜ ì„¤ê³„",
                    "ì•”ë³´í—˜ ì„¤ê³„ (íŠ¹ì•½, ì§„ë‹¨ê¸ˆ ë“±)",
                    "ì–´ë¦°ì´ë³´í—˜ ì„¤ê³„ (íƒœì•„, ì„±ì¥ê¸° ë“±)",
                    "ìš´ì „ìë³´í—˜ ì„¤ê³„",
                    "ì—°ê¸ˆë³´í—˜ ì„¤ê³„ (ë…¸í›„, ì ˆì„¸ ë“±)",
                    "ì§ì¥ì¸ ë³´í—˜ ì„¤ê³„ (ì†Œë“ë³´ì¥, ê±´ê°• ë“±)",
                    "ì‹ í˜¼ë¶€ë¶€ ë³´í—˜ ì„¤ê³„ (ê°€ì¡± ë³´ì¥, ì¬ë¬´ ê³„íš ë“±)",
                    "ìì˜ì—…ì ë³´í—˜ ì„¤ê³„ (ì‚¬ì—…ì, ìƒí•´ë³´í—˜ ë“±)",
                    "40ëŒ€ ë³´í—˜ (íŠ¹ì • ì—°ë ¹ëŒ€ ë‹ˆì¦ˆ)",
                    "ê° ë³´í—˜ì‚¬ë³„ ìµœì‹  ìƒí’ˆ ìƒì„¸ ì •ë³´",
                    "í•µì‹¬ ì•½ê´€ ë° íŠ¹ì•½ ì •ë³´",
                    "ë³´í—˜ë£Œ ì‚°ì¶œ ê·¼ê±° ë°ì´í„°",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹˜ ìê²©ê³¼ ì •ì˜ ë‚´ë¦¬ê¸°ì˜ ì¤‘ìš”ì„±",
                    "ê³ ê° ê´€ì ì—ì„œì˜ ê°€ê²©ê³¼ ê°€ì¹˜ì˜ ì¦ëª…",
                    "ì„¸ì¼ì¦ˆ ì „ëµê³¼ íœ´ë¨¸ë‹ˆí‹°ì˜ ì¤‘ìš”ì„±",
                    "ì •ì˜ ë‚´ë¦¬ê¸°ì˜ ì¤‘ìš”ì„±",
                    "ë³´í—˜ì—…ì˜ ì •ì˜",
                    "ì¸ìƒì˜ ì¤‘ìš”í•œ ìˆœê°„",
                    "ì„¸ì¼ì¦ˆì˜ ì •ì˜",
                    "ìŠ¤í† ë¦¬í…”ë§ì˜ ê°œë…",
                    "ì •ì˜ ë‚´ë¦¬ê¸° ê¸°ë²•",
                    "ë³´í—˜ì˜ ì •ì˜",
                    "ê°€ì†”ë¦¬ë‹ˆ ë²•ì¹™",
                    "ê³ ê°ì˜ ì–¸ì–´ íŒ¨í„´",
                    "ì„¸ì¼ì¦ˆ ì‹¤íŒ¨ì˜ ë‘ ë²ˆì§¸ ê°œë…",
                    "ì„¸ì¼ì¦ˆë§¨ì˜ ì¥ì  ìë‘",
                    "ì¬ëŠ¥ ê¸°ë¶€ì˜ ì˜ë¯¸",
                    "ê³ ê°ì„ ìœ„í•œ ì„¸ ê°€ì§€",
                    "ë³´í—˜ ì„¸ì¼ì¦ˆ ì „ëµ",
                    "ê³ ê°ì˜ ë‹ˆì¦ˆì™€ ê°€ê²©",
                    "ê³ ê° ê´€ì ì—ì„œì˜ ê°€ê²©",
                    "ë¦¬ìŠ¤í¬ í•´ê²° ë°©ë²•",
                    "ê°€ì†”ë¦°ì˜ ì¢…ë¥˜",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹­ê³¼ ì„¸ì¼ì¦ˆì˜ ì°¨ì´",
                    "ì„¸ì¼ì¦ˆ ì„±ê³µì„ ìœ„í•œ íŒ¨í„´ ì½”ì¹­ì˜ ì¤‘ìš”ì„±",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹˜ì˜ ë¹„ì „ê³¼ ìŠ¤í† ë¦¬í…”ë§ì˜ ì¤‘ìš”ì„±",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹­ì˜ ì„±ì¥",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹­ê³¼ ì„¸ì¼ì¦ˆì˜ ì°¨ì´",
                    "ì •ì˜ ë‚´ë¦¬ê¸°ì˜ ë°©ë²•",
                    "ì½”ì¹­ì˜ ì •ì˜",
                    "ì½”ì¹­ì˜ ì •ì˜",
                    "ë§ˆì¼€íŒ…ì˜ ê°•ì ",
                    "ë©”ì‹œì§€ í˜•ì˜ ê°•ì ê³¼ ì•½ì ",
                    "íŒ¨í„´ ì½”ì¹­ì˜ ì¥ì ",
                    "ì„¸ì¼ì¦ˆ ì„±ê³µì˜ í”„ë¡œì„¸ìŠ¤í˜• ì‚¬ê³ ",
                    "ì„¸ì¼ì¦ˆì˜ ê¸°ë³¸",
                    "ì„¸ì¼ì¦ˆì˜ ë„¤ ê°€ì§€ íŒ¨í„´",
                    "ê³µê°ì˜ ëŠ¥ë ¥ê³¼ ìì•„ ìš•ë§ì˜ ëŠ¥ë ¥",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹˜ì˜ ë¹„ì „",
                    "ë‚´í–¥ì ì¸ ì‚¬ëŒì˜ ì—­ëŸ‰"
                ]
                num_blog_results_per_query = int(input(f"ê° í‚¤ì›Œë“œë‹¹ ëª‡ ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ìµœëŒ€ 100ê°œ, ê¸°ë³¸ê°’: 10): ") or "10")
                if num_blog_results_per_query > 100: num_blog_results_per_query = 100
                elif num_blog_results_per_query < 1: num_blog_results_per_query = 10
                
                blog_docs = retrieve_and_split_naver_blog_contents(search_queries_list, num_blog_results_per_query)
                final_documents_to_chunk.extend(blog_docs)
            elif indexing_choice == '2' or indexing_choice == 'ë¡œì»¬ ë¬¸ì„œ':
                local_docs = load_local_documents_and_split(LOCAL_DOCS_PATH)
                final_documents_to_chunk.extend(local_docs)
            elif indexing_choice == '3' or indexing_choice == 'ë‘˜ ë‹¤':
                search_queries_list = [
                    "ë³´í—˜ìƒí’ˆ ë¹„êµ",
                    "ë³´ì¥ ë¶„ì„",
                    "ë³´í—˜ ë¦¬ëª¨ë¸ë§",
                    "ìƒëª…ë³´í—˜ ì„¤ê³„ (ì¢…ì‹ /ì •ê¸°/ë³€ì•¡ ë“±)",
                    "ì‹¤ë¹„ë³´í—˜ ì„¤ê³„",
                    "ì•”ë³´í—˜ ì„¤ê³„ (íŠ¹ì•½, ì§„ë‹¨ê¸ˆ ë“±)",
                    "ì–´ë¦°ì´ë³´í—˜ ì„¤ê³„ (íƒœì•„, ì„±ì¥ê¸° ë“±)",
                    "ìš´ì „ìë³´í—˜ ì„¤ê³„",
                    "ì—°ê¸ˆë³´í—˜ ì„¤ê³„ (ë…¸í›„, ì ˆì„¸ ë“±)",
                    "ì§ì¥ì¸ ë³´í—˜ ì„¤ê³„ (ì†Œë“ë³´ì¥, ê±´ê°• ë“±)",
                    "ì‹ í˜¼ë¶€ë¶€ ë³´í—˜ ì„¤ê³„ (ê°€ì¡± ë³´ì¥, ì¬ë¬´ ê³„íš ë“±)",
                    "ìì˜ì—…ì ë³´í—˜ ì„¤ê³„ (ì‚¬ì—…ì, ìƒí•´ë³´í—˜ ë“±)",
                    "40ëŒ€ ë³´í—˜ (íŠ¹ì • ì—°ë ¹ëŒ€ ë‹ˆì¦ˆ)",
                    "ê° ë³´í—˜ì‚¬ë³„ ìµœì‹  ìƒí’ˆ ìƒì„¸ ì •ë³´",
                    "í•µì‹¬ ì•½ê´€ ë° íŠ¹ì•½ ì •ë³´",
                    "ë³´í—˜ë£Œ ì‚°ì¶œ ê·¼ê±° ë°ì´í„°",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹˜ ìê²©ê³¼ ì •ì˜ ë‚´ë¦¬ê¸°ì˜ ì¤‘ìš”ì„±",
                    "ê³ ê° ê´€ì ì—ì„œì˜ ê°€ê²©ê³¼ ê°€ì¹˜ì˜ ì¦ëª…",
                    "ì„¸ì¼ì¦ˆ ì „ëµê³¼ íœ´ë¨¸ë‹ˆí‹°ì˜ ì¤‘ìš”ì„±",
                    "ì •ì˜ ë‚´ë¦¬ê¸°ì˜ ì¤‘ìš”ì„±",
                    "ë³´í—˜ì—…ì˜ ì •ì˜",
                    "ì¸ìƒì˜ ì¤‘ìš”í•œ ìˆœê°„",
                    "ì„¸ì¼ì¦ˆì˜ ì •ì˜",
                    "ìŠ¤í† ë¦¬í…”ë§ì˜ ê°œë…",
                    "ì •ì˜ ë‚´ë¦¬ê¸° ê¸°ë²•",
                    "ë³´í—˜ì˜ ì •ì˜",
                    "ê°€ì†”ë¦¬ë‹ˆ ë²•ì¹™",
                    "ê³ ê°ì˜ ì–¸ì–´ íŒ¨í„´",
                    "ì„¸ì¼ì¦ˆ ì‹¤íŒ¨ì˜ ë‘ ë²ˆì§¸ ê°œë…",
                    "ì„¸ì¼ì¦ˆë§¨ì˜ ì¥ì  ìë‘",
                    "ì¬ëŠ¥ ê¸°ë¶€ì˜ ì˜ë¯¸",
                    "ê³ ê°ì„ ìœ„í•œ ì„¸ ê°€ì§€",
                    "ë³´í—˜ ì„¸ì¼ì¦ˆ ì „ëµ",
                    "ê³ ê°ì˜ ë‹ˆì¦ˆì™€ ê°€ê²©",
                    "ê³ ê° ê´€ì ì—ì„œì˜ ê°€ê²©",
                    "ë¦¬ìŠ¤í¬ í•´ê²° ë°©ë²•",
                    "ê°€ì†”ë¦°ì˜ ì¢…ë¥˜",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹­ê³¼ ì„¸ì¼ì¦ˆì˜ ì°¨ì´",
                    "ì„¸ì¼ì¦ˆ ì„±ê³µì„ ìœ„í•œ íŒ¨í„´ ì½”ì¹­ì˜ ì¤‘ìš”ì„±",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹˜ì˜ ë¹„ì „ê³¼ ìŠ¤í† ë¦¬í…”ë§ì˜ ì¤‘ìš”ì„±",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹­ì˜ ì„±ì¥",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹­ê³¼ ì„¸ì¼ì¦ˆì˜ ì°¨ì´",
                    "ì •ì˜ ë‚´ë¦¬ê¸°ì˜ ë°©ë²•",
                    "ì½”ì¹­ì˜ ì •ì˜",
                    "ì½”ì¹­ì˜ ì •ì˜",
                    "ë§ˆì¼€íŒ…ì˜ ê°•ì ",
                    "ë©”ì‹œì§€ í˜•ì˜ ê°•ì ê³¼ ì•½ì ",
                    "íŒ¨í„´ ì½”ì¹­ì˜ ì¥ì ",
                    "ì„¸ì¼ì¦ˆ ì„±ê³µì˜ í”„ë¡œì„¸ìŠ¤í˜• ì‚¬ê³ ",
                    "ì„¸ì¼ì¦ˆì˜ ê¸°ë³¸",
                    "ì„¸ì¼ì¦ˆì˜ ë„¤ ê°€ì§€ íŒ¨í„´",
                    "ê³µê°ì˜ ëŠ¥ë ¥ê³¼ ìì•„ ìš•ë§ì˜ ëŠ¥ë ¥",
                    "ì„¸ì¼ì¦ˆ ì½”ì¹˜ì˜ ë¹„ì „",
                    "ë‚´í–¥ì ì¸ ì‚¬ëŒì˜ ì—­ëŸ‰"
                ]
                num_blog_results_per_query = int(input(f"ê° í‚¤ì›Œë“œë‹¹ ëª‡ ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ìµœëŒ€ 100ê°œ, ê¸°ë³¸ê°’: 10): ") or "10")
                if num_blog_results_per_query > 100: num_blog_results_per_query = 100
                elif num_blog_results_per_query < 1: num_blog_results_per_query = 10

                blog_docs = retrieve_and_split_naver_blog_contents(search_queries_list, num_blog_results_per_query)
                local_docs = load_local_documents_and_split(LOCAL_DOCS_PATH)
                final_documents_to_chunk.extend(blog_docs)
                final_documents_to_chunk.extend(local_docs)
            else:
                print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            if final_documents_to_chunk:
                print(f"\n--- ì´ {len(final_documents_to_chunk)}ê°œì˜ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ---")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=100,
                    length_function=len,
                    add_start_index=True,
                )
                all_chunks = text_splitter.split_documents(final_documents_to_chunk)
                print(f"ì´ {len(all_chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                embed_and_store(all_chunks, CHROMA_DB_PATH, embeddings)
            else:
                print("â›” ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì„ íƒ ë° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            # ì¸ë±ì‹± ì™„ë£Œ í›„ ë‹¤ìŒ ì„ íƒì§€ë¥¼ ìœ„í•´ final_documents_to_chunk ì´ˆê¸°í™”
            final_documents_to_chunk = []

        print("\n--- RAG ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ---")
                
    except Exception as e:
        print(f"\nğŸš¨ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # Selenium ë“œë¼ì´ë²„ê°€ ë‚¨ì•„ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„ (ê° ìŠ¤í¬ë˜í•‘ë§ˆë‹¤ ë‹«ì§€ë§Œ, ì˜ˆì™¸ ìƒí™© ëŒ€ë¹„)
        # global driver # ì´ì „ ì „ì—­ ë“œë¼ì´ë²„ ë°©ì‹ì¼ ë•Œ í•„ìš”í–ˆìœ¼ë‚˜, ì´ì œëŠ” ê° í•¨ìˆ˜ ë‚´ì—ì„œ ì²˜ë¦¬
        # if driver: 
        #     quit_driver_safely(driver) # ì´ ë¼ì¸ì€ í˜„ì¬ ì½”ë“œ êµ¬ì¡°ìƒ í•„ìš” ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        pass # í˜„ì¬ êµ¬ì¡°ì—ì„œëŠ” í•„ìš” ì—†ìŒ