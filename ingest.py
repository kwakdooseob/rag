# -*- coding: utf-8 -*-
from dotenv import load_dotenv
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
import sys
import time
import shutil
import requests
from bs4 import BeautifulSoup
import json
import re

# Selenium ê´€ë ¨ ì„í¬íŠ¸
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException, StaleElementReferenceException

# Langchain Document Loaders for local files
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader, CSVLoader
# Note: For Unstructured loaders (e.g., UnstructuredWordDocumentLoader, UnstructuredExcelLoader),
# you generally need the 'unstructured' library installed.
# For .pdf, 'pypdf'
# For .docx, 'python-docx'
# For .csv, 'pandas' (often implicitly used by CSVLoader or via unstructured)
# For .xlsx, 'openpyxl' (often implicitly used by UnstructuredExcelLoader or via unstructured)


# Load environment variables from .env file
load_dotenv()

# --- Environment Variables ---
OLLAMA_HOST = os.environ.get('OLLAMA_HOST', 'http://127.0.0.1:11434')
OLLAMA_EMBEDDING_MODEL = os.environ.get('OLLAMA_EMBEDDING_MODEL', 'nomic-embed-text')
CHROMA_DB_PATH = os.environ.get('CHROMA_DB_PATH', './chroma_db')

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

LOCAL_DOCS_PATH = os.environ.get('LOCAL_DOCS_PATH', './local_documents')
NAVER_BLOG_QUERIES_FILE = os.environ.get("NAVER_BLOG_QUERIES_FILE", "./naver_blog_queries.txt") # .envì—ì„œ ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ ë¡œë“œ

# ChromeDriver path configuration
CHROMEDRIVER_PATH = os.path.join(os.path.dirname(__file__), 'chromedriver.exe') 

print(f"Ollama Host: {OLLAMA_HOST}")
print(f"Embedding Model: {OLLAMA_EMBEDDING_MODEL}")
print(f"Chroma DB Path: {CHROMA_DB_PATH}")
print(f"Naver Client ID (first 5 chars): {NAVER_CLIENT_ID[:5] if NAVER_CLIENT_ID else 'Not Set'}")
print(f"ChromeDriver Path: {CHROMEDRIVER_PATH}")
print(f"Local Documents Path: {LOCAL_DOCS_PATH}")
print(f"Naver Blog Queries File: {NAVER_BLOG_QUERIES_FILE}")


# Validate Naver API credentials
if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
    print("âŒ ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

# Naver Search API Configuration
NAVER_BLOG_SEARCH_API_URL = "https://openapi.naver.com/v1/search/blog.json"
NAVER_API_HEADERS = {
    "X-Naver-Client-Id": NAVER_CLIENT_ID,
    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
}

# --- Selenium WebDriver ì´ˆê¸°í™”/ì¢…ë£Œ í•¨ìˆ˜ ---
def get_new_driver():
    # print("Initializing Chrome WebDriver for scraping...") # ë„ˆë¬´ ìì£¼ ì¶œë ¥ë  ìˆ˜ ìˆì–´ ì£¼ì„ ì²˜ë¦¬
    options = webdriver.ChromeOptions()
    options.add_argument('headless')
    options.add_argument('window-size=1920x1080')
    options.add_argument('disable-gpu')
    options.add_argument('no-sandbox')
    options.add_argument('disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-logging"])

    try:
        service = Service(executable_path=CHROMEDRIVER_PATH)
        driver_instance = webdriver.Chrome(service=service, options=options)
        # print("Chrome WebDriver initialized.") # ë„ˆë¬´ ìì£¼ ì¶œë ¥ë  ìˆ˜ ìˆì–´ ì£¼ì„ ì²˜ë¦¬
        return driver_instance
    except WebDriverException as e:
        print(f"âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print(f"ğŸ’¡ ChromeDriver ({CHROMEDRIVER_PATH})ê°€ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆê³ , Chrome ë¸Œë¼ìš°ì € ë²„ì „ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None

def quit_driver_safely(driver_instance):
    if driver_instance:
        try:
            driver_instance.quit()
            # print("Chrome WebDriver closed.") # ë„ˆë¬´ ìì£¼ ì¶œë ¥ë  ìˆ˜ ìˆì–´ ì£¼ì„ ì²˜ë¦¬
        except Exception as e:
            print(f"âš ï¸ WebDriver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def scrape_blog_content_selenium(url: str) -> str:
    if "n.news.naver.com" in url or "tistory.com" in url or "daum.net" in url or "post.naver.com" in url:
        print(f"   â„¹ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤/íƒ€ í”Œë«í¼/ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ë§í¬ëŠ” ìŠ¤í¬ë˜í•‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {url}")
        return ""
    
    if not "blog.naver.com" in url:
        print(f"   â„¹ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë„ë©”ì¸ì´ ì•„ë‹ˆë¯€ë¡œ ìŠ¤í¬ë˜í•‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {url}")
        return ""

    browser = None
    content_text = ""

    try:
        browser = get_new_driver()
        if browser is None:
            return ""

        print(f"   ğŸŒ ë¸Œë¼ìš°ì €: {url} ë¡œë“œ ì¤‘...")
        browser.get(url)
        WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        time.sleep(2)

        target_html_source = browser.page_source
        current_url = browser.current_url

        try:
            iframe_ids = ['mainFrame', 'screenFrame'] 
            iframe_element = None
            for iframe_id in iframe_ids:
                try:
                    iframe_element = WebDriverWait(browser, 3).until(
                        EC.presence_of_element_located((By.ID, iframe_id))
                    )
                    if iframe_element:
                        break
                except TimeoutException:
                    continue

            if iframe_element:
                iframe_src = iframe_element.get_attribute('src')
                if iframe_src.startswith('//'):
                    iframe_url = "https:" + iframe_src
                elif iframe_src.startswith('/'):
                    iframe_url = "https://blog.naver.com" + iframe_src
                else:
                    iframe_url = iframe_src

                print(f"   ğŸ” iframe (ID: {iframe_element.get_attribute('id')}) ë°œê²¬. URL: {iframe_url} ì „í™˜ ì‹œë„ ì¤‘...")
                
                if iframe_url and "blog.naver.com" in iframe_url and iframe_url != current_url:
                    browser.switch_to.frame(iframe_element)
                    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                    time.sleep(1)
                    target_html_source = browser.page_source
                    print(f"   âœ… iframe ë‚´ë¶€ë¡œ ì „í™˜ ì„±ê³µ.")
                else:
                    print(f"   âš ï¸ iframe URLì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë©”ì¸ í˜ì´ì§€ì™€ ë™ì¼. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©: {iframe_url}")
            else:
                print(f"   âš ï¸ í˜ì´ì§€ì—ì„œ ì í•©í•œ iframeì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©.")
        except StaleElementReferenceException:
            print(f"   âš ï¸ StaleElementReferenceException ë°œìƒ. í˜ì´ì§€ ë¦¬ë¡œë“œ í›„ ì¬ì‹œë„.")
            browser.get(url)
            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
            time.sleep(2)
            target_html_source = browser.page_source
        except Exception as e:
            print(f"   âš ï¸ iframe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë©”ì¸ í˜ì´ì§€ HTML ì‚¬ìš©.")
            target_html_source = browser.page_source

        soup_to_parse = BeautifulSoup(target_html_source, 'html.parser')

        content_selectors = [
            'div.se-main-container',
            'div.se-viewer',
            'div#postListBody',
            'div#post-view',
            'div.blog_post',
            'div.se_component_wrap',
            'div.post-content',
            'div.post_ct',
            'div.blog_content',
            'div.area_view',
            'div.section_article',
            'div[id^="post-area"]',
            'div.se_component'
        ]
        
        content_element_soup = None
        for selector in content_selectors:
            try:
                content_element_soup = soup_to_parse.select_one(selector)
                if content_element_soup:
                    print(f"   âœ… ë³¸ë¬¸ ì½˜í…ì¸  ì˜ì—­ ë°œê²¬ (CSS: {selector})")
                    break
            except Exception:
                continue
        
        if content_element_soup:
            for unwanted_tag in content_element_soup(['script', 'style', 'noscript', 'img', 'a', 'iframe', 'video', 'audio', 'header', 'footer', 'nav', 'form']):
                unwanted_tag.extract()
            
            content_text = content_element_soup.get_text(separator='\n', strip=True)
            content_text = re.sub(r'\n\s*\n', '\n\n', content_text).strip()
            
        else:
            print(f"   âš ï¸ íŠ¹ì • ë³¸ë¬¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ íƒœê·¸ ì¶”ì¶œ ì‹œë„: {url}")
            
            possible_main_content_area = soup_to_parse.find('div', class_='se-main-area') or \
                                         soup_to_parse.find('div', id='viewTypeSelector') or \
                                         soup_to_parse.find('div', class_='post-content') or \
                                         soup_to_parse.find('div', class_='wrap_blogview') or \
                                         soup_to_parse.find('body')

            if possible_main_content_area:
                for unwanted_tag in possible_main_content_area(['script', 'style', 'noscript', 'img', 'a', 'iframe', 'video', 'audio', 'header', 'footer', 'nav', 'form']):
                    unwanted_tag.extract()
                
                paragraphs = possible_main_content_area.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'li', 'span', 'strong'])
                if paragraphs:
                    content_text = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                    content_text = re.sub(r'\n\s*\n', '\n\n', content_text).strip()
                    if content_text:
                        print(f"   â„¹ï¸ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ íƒœê·¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¼ë¶€ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
            
            if not content_text:
                print(f"   âŒ ë³¸ë¬¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ìµœì¢… ì‹¤íŒ¨: {url}")

        return content_text.strip()
            
    except TimeoutException:
        print(f"   âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼ ({url})")
        return ""
    except WebDriverException as e:
        print(f"   âš ï¸ WebDriver ì˜¤ë¥˜ ({url}): {e}")
        return ""
    except Exception as e:
        print(f"   âš ï¸ ë¸”ë¡œê·¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({url}): {e}. íƒ€ì…: {type(e).__name__}")
        return ""
    finally:
        try:
            if browser:
                browser.switch_to.default_content() 
        except Exception as e:
            print(f"   âš ï¸ ë“œë¼ì´ë²„ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        quit_driver_safely(browser)


def retrieve_and_split_naver_blog_contents(queries: list, num_results_per_query: int = 5):
    if not queries:
        print("ğŸ’¡ ê²€ìƒ‰í•  ì¿¼ë¦¬ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    print("\n--- 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„° ê²€ìƒ‰ ë° ë¡œë“œ ë° ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ---")
    all_documents = []
    
    for query in queries:
        query = query.strip()
        if not query: continue

        print(f"\n'{query}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘...")
        current_query_results = []
        start_index = 1 

        while len(current_query_results) < num_results_per_query:
            display_count = min(num_results_per_query - len(current_query_results), 100)
            if display_count <= 0:
                break

            params = {
                "query": query,
                "display": display_count,
                "start": start_index,
                "sort": "sim"
            }
            
            try:
                response = requests.get(NAVER_BLOG_SEARCH_API_URL, headers=NAVER_API_HEADERS, params=params, timeout=10)
                response.raise_for_status()
                search_data = json.loads(response.text)
                
                items = search_data.get('items', [])
                
                unique_items = []
                # í˜„ì¬ ê²€ìƒ‰ ì„¸ì…˜ ë‚´ì—ì„œë§Œ ì¤‘ë³µì„ ë°©ì§€í•©ë‹ˆë‹¤.
                existing_links_in_session = {item.get('link') for item in current_query_results}
                
                for item in items:
                    clean_link = item.get('link')
                    if clean_link and not clean_link.startswith(('http://', 'https://')):
                        continue
                    if clean_link not in existing_links_in_session:
                        unique_items.append(item)
                        existing_links_in_session.add(clean_link)
                current_query_results.extend(unique_items)
                
                if not items or len(items) < display_count:
                    break
                
                start_index += display_count
                time.sleep(0.1)
            except requests.exceptions.RequestException as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ìš”ì²­ ì˜¤ë¥˜ ('{query}'): {e}")
                break
            except json.JSONDecodeError as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜ ('{query}'): {e}")
                break
            except Exception as e:
                print(f"   âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ('{query}'): {e}")
                break
        
        if not current_query_results:
            print(f"'{query}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        for i, res in enumerate(current_query_results):
            blog_link = res.get("link")
            
            title_clean = res.get('title', '').replace("<b>", "").replace("</b>", "").replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
            desc_clean = res.get('description', '').replace("<b>", "").replace("</b>", "").replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")

            if not blog_link or not blog_link.startswith(('http://', 'https://')):
                print(f"   âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë¸”ë¡œê·¸ ë§í¬ ê±´ë„ˆë›°ê¸°: {blog_link}")
                content = f"ì œëª©: {title_clean}\nì„¤ëª…: {desc_clean}\n(ìœ íš¨í•˜ì§€ ì•Šì€ ë§í¬)"
                doc = Document(page_content=content, metadata={"source": "invalid_link", "title": title_clean, "query": query, "pub_date": res.get("postdate", ""), "source_type": "blog"})
                all_documents.append(doc)
                continue 

            print(f"   ğŸ”— '{title_clean}' ({blog_link}) ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹œë„ ì¤‘...")
            full_blog_content = scrape_blog_content_selenium(blog_link) 
            
            content = f"ì œëª©: {title_clean}\nì„¤ëª…: {desc_clean}\n"
            if full_blog_content:
                content += f"ë³¸ë¬¸:\n{full_blog_content}"
            else:
                content += "(ë³¸ë¬¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ)"

            doc = Document(
                page_content=content,
                metadata={
                    "source": blog_link,
                    "title": title_clean,
                    "query": query,
                    "pub_date": res.get("postdate", ""),
                    "source_type": "blog" # ë©”íƒ€ë°ì´í„°ì— source_type ì¶”ê°€
                }
            )
            all_documents.append(doc)
            time.sleep(1)
        print(f"'{query}'ì— ëŒ€í•´ {len(current_query_results)}ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        time.sleep(2)

    if not all_documents:
        print("â›” ê²€ìƒ‰ ë° ìŠ¤í¬ë˜í•‘ëœ ë¬¸ì„œê°€ ì—†ì–´ ì¸ë±ì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë„¤ì´ë²„ API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return []

    return all_documents


def load_local_documents_and_split(local_docs_path: str):
    """
    Loads documents from a local directory.
    Supports .txt, .pdf, .docx, .csv.
    """
    print(f"\n--- ë¡œì»¬ ë¬¸ì„œ '{local_docs_path}' ë¡œë“œ ë° ë¶„í•  ---")
    
    if not os.path.exists(local_docs_path):
        print(f"âŒ ë¡œì»¬ ë¬¸ì„œ ê²½ë¡œ '{local_docs_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë”ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return []

    documents = []
    
    loader_mapping = {
        ".txt": TextLoader,
        ".pdf": PyPDFLoader,
        ".docx": Docx2txtLoader, 
        ".csv": CSVLoader,
        # ".xlsx": UnstructuredExcelLoader # Requires 'unstructured' and 'openpyxl'
    }
    
    for root, _, files in os.walk(local_docs_path):
        for file_name in files:
            file_path = os.path.join(root, file_name)
            file_extension = os.path.splitext(file_name)[1].lower()

            loader_class = loader_mapping.get(file_extension)
            if loader_class:
                print(f"   ğŸ“„ ë¡œì»¬ ë¬¸ì„œ ë¡œë“œ ì¤‘: {file_name} (ìœ í˜•: {file_extension})")
                try:
                    loader = None
                    if file_extension == '.txt':
                        # Try multiple encodings for TXT files
                        encodings = ['utf-8', 'cp949', 'euc-kr']
                        for enc in encodings:
                            try:
                                temp_loader = TextLoader(file_path, encoding=enc)
                                _ = temp_loader.load() # Test load
                                loader = temp_loader
                                print(f"      â„¹ï¸ '{file_name}' ({enc} ì¸ì½”ë”© ì„±ê³µ)")
                                break # Found a working encoding
                            except UnicodeDecodeError:
                                print(f"      â„¹ï¸ '{file_name}' ({enc} ì¸ì½”ë”© ì‹¤íŒ¨), ë‹¤ìŒ ì‹œë„...")
                            except Exception as e:
                                print(f"   âŒ '{file_name}' í…ìŠ¤íŠ¸ ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({enc}): {e}")
                                break # Stop trying encodings for this file on other errors
                        if loader is None: # If no suitable loader was found after all encoding attempts
                            print(f"   âŒ '{file_name}' ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨. íŒŒì¼ ê±´ë„ˆë›°ê¸°.")
                            continue # Skip this file and go to next file_name in loop
                    else: # For other document types, just use the the loader_class
                        loader = loader_class(file_path)

                    if loader: # Only proceed if a loader was successfully determined
                        loaded_docs = loader.load()
                        for doc in loaded_docs:
                            doc.metadata["source"] = file_path
                            doc.metadata["title"] = file_name
                            doc.metadata["source_type"] = "local_doc" # Add source_type metadata
                            documents.append(doc)
                        print(f"   âœ… ë¡œì»¬ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {file_name}")
                except Exception as e:
                    print(f"   âŒ ë¡œì»¬ ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_name}): {e}")
            else:
                print(f"   âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ê±´ë„ˆë›°ê¸°: {file_name}")
    
    if not documents:
        print(f"ğŸ’¡ ë¡œì»¬ ë¬¸ì„œ í´ë” '{local_docs_path}'ì— ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    return documents # Return Document objects


def embed_and_store(chunks, db_path, embeddings_function):
    print("\n--- 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/ì—…ë°ì´íŠ¸ ---")
    try:
        vectorstore = None
        
        # Check if the DB directory exists and is not empty
        if os.path.exists(db_path) and os.listdir(db_path):
            try:
                # Try to load existing DB
                vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings_function)
                print(f"âœ… ê¸°ì¡´ Chroma DB '{db_path}' ë¡œë“œ ì™„ë£Œ. ìƒˆ ì²­í¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                
                # Basic duplicate prevention: check if source already exists in DB
                existing_sources = set()
                try:
                    # Fetch only metadata to avoid loading all document contents into memory, which can be slow for large DBs.
                    # We assume 'source' metadata is unique enough for deduplication.
                    existing_docs_in_db = vectorstore.get(include=['metadatas'])
                    for meta in existing_docs_in_db['metadatas']:
                        existing_sources.add(meta.get('source'))
                except Exception as e:
                    print(f"   âš ï¸ ê¸°ì¡´ DB ë¬¸ì„œ ì†ŒìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

                new_chunks = []
                for chunk in chunks:
                    if chunk.metadata.get('source') not in existing_sources:
                        new_chunks.append(chunk)
                    else:
                        print(f"   â„¹ï¸ ì¤‘ë³µëœ ë¬¸ì„œ ì²­í¬ ê±´ë„ˆë›°ê¸°: {chunk.metadata.get('source')} - {chunk.metadata.get('title')}")
                
                if new_chunks:
                    vectorstore.add_documents(new_chunks)
                    print(f"âœ… {len(new_chunks)}ê°œì˜ ìƒˆ ì²­í¬ê°€ '{db_path}' ê²½ë¡œì˜ Chroma DBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("ğŸ’¡ ìƒˆë¡œ ì¶”ê°€í•  ì¤‘ë³µë˜ì§€ ì•Šì€ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                print(f"   âš ï¸ ê¸°ì¡´ Chroma DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({e}). ìƒˆë¡œìš´ DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                # If loading existing DB fails, create a new one
                vectorstore = Chroma.from_documents(
                    documents=chunks,
                    embedding=embeddings_function,
                    persist_directory=db_path
                )
                print(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ìƒˆë¡œìš´ Chroma DB '{db_path}'ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"ğŸ’¡ Chroma DB '{db_path}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            # If DB does not exist or is empty, create a new one
            vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings_function,
                persist_directory=db_path
            )
            print(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ìƒˆë¡œìš´ Chroma DB '{db_path}'ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ Chroma DBì— ë¬¸ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
    finally:
        # ChromaDB 0.4.x onwards automatically persists, so explicit .persist() is not strictly needed.
        pass


if __name__ == "__main__":
    print("--- RAG ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ---")
    
    try:
        embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        print(f"Ollama Embedding Model '{OLLAMA_EMBEDDING_MODEL}' ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ Ollama Embedding Model ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ì§€ì •ëœ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì˜ˆ: ollama serve ì‹¤í–‰ í›„, ollama pull {OLLAMA_EMBEDDING_MODEL}")
        sys.exit(1)

    try:
        # .envì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        def load_queries_from_file(file_path):
            queries = []
            if not os.path.exists(file_path):
                print(f"âš ï¸ ê²½ê³ : ì¿¼ë¦¬ íŒŒì¼ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        query = line.strip()
                        if query and not query.startswith('#'): # ë¹ˆ ì¤„ ë° ì£¼ì„ ì œê±°
                            queries.append(query)
                print(f"âœ… ì¿¼ë¦¬ íŒŒì¼ '{file_path}'ì—ì„œ {len(queries)}ê°œì˜ í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ ì¿¼ë¦¬ íŒŒì¼ '{file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return queries
        
        search_queries_list = load_queries_from_file(NAVER_BLOG_QUERIES_FILE)

        if not search_queries_list:
            print("âš ï¸ NAVER_BLOG_QUERIES_FILEì— ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¸”ë¡œê·¸ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        while True:
            final_documents_to_process = [] # ì²­í¬ë¡œ ë¶„í• í•  ìµœì¢… Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
            indexing_choice = input(
                "\nì–´ë–¤ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ ì¸ë±ì‹±í• ê¹Œìš”? (1: ë¸”ë¡œê·¸, 2: ë¡œì»¬ ë¬¸ì„œ, 3: ë‘˜ ë‹¤, exit: ì¢…ë£Œ): "
            ).strip().lower()

            if indexing_choice == 'exit':
                break
            elif indexing_choice == '1' or indexing_choice == 'ë¸”ë¡œê·¸':
                if not search_queries_list:
                    print("âŒ ë¸”ë¡œê·¸ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•„ ë¸”ë¡œê·¸ ì¸ë±ì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                num_blog_results_per_query = int(input(f"ê° í‚¤ì›Œë“œë‹¹ ëª‡ ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ìµœëŒ€ 100ê°œ, ê¸°ë³¸ê°’: 10): ") or "10")
                if num_blog_results_per_query > 100: num_blog_results_per_query = 100
                elif num_blog_results_per_query < 1: num_blog_results_per_query = 10
                
                print(f"\n--- ë¯¸ë¦¬ ì§€ì •ëœ ë¸”ë¡œê·¸ í‚¤ì›Œë“œë¡œ ì¸ë±ì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
                print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_queries_list}")
                print(f"ê° í‚¤ì›Œë“œë‹¹ ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜: {num_blog_results_per_query}")

                blog_docs = retrieve_and_split_naver_blog_contents(search_queries_list, num_blog_results_per_query)
                final_documents_to_process.extend(blog_docs)
            elif indexing_choice == '2' or indexing_choice == 'ë¡œì»¬ ë¬¸ì„œ':
                local_docs = load_local_documents_and_split(LOCAL_DOCS_PATH)
                final_documents_to_process.extend(local_docs)
            elif indexing_choice == '3' or indexing_choice == 'ë‘˜ ë‹¤':
                if not search_queries_list:
                    print("âŒ ë¸”ë¡œê·¸ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•„ ë¸”ë¡œê·¸ ì¸ë±ì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ ë¬¸ì„œë§Œ ì¸ë±ì‹±í•©ë‹ˆë‹¤.")
                    local_docs = load_local_documents_and_split(LOCAL_DOCS_PATH)
                    final_documents_to_process.extend(local_docs)
                    if not final_documents_to_process:
                        print("â›” ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì„ íƒ ë° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
                    continue
                    
                num_blog_results_per_query = int(input(f"ê° í‚¤ì›Œë“œë‹¹ ëª‡ ê°œì˜ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ìµœëŒ€ 100ê°œ, ê¸°ë³¸ê°’: 10): ") or "10")
                if num_blog_results_per_query > 100: num_blog_results_per_query = 100
                elif num_blog_results_per_query < 1: num_blog_results_per_query = 10

                print(f"\n--- ë¯¸ë¦¬ ì§€ì •ëœ ë¸”ë¡œê·¸ í‚¤ì›Œë“œë¡œ ì¸ë±ì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
                print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_queries_list}")
                print(f"ê° í‚¤ì›Œë“œë‹¹ ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜: {num_blog_results_per_query}")

                blog_docs = retrieve_and_split_naver_blog_contents(search_queries_list, num_blog_results_per_query)
                local_docs = load_local_documents_and_split(LOCAL_DOCS_PATH)
                final_documents_to_process.extend(blog_docs)
                final_documents_to_process.extend(local_docs)
            else:
                print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            if final_documents_to_process:
                print(f"\n--- ì´ {len(final_documents_to_process)}ê°œì˜ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ---")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=100,
                    length_function=len,
                    add_start_index=True,
                )
                all_chunks = text_splitter.split_documents(final_documents_to_process)
                print(f"ì´ {len(all_chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                embed_and_store(all_chunks, CHROMA_DB_PATH, embeddings)
            else:
                print("â›” ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì„ íƒ ë° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            # ì¸ë±ì‹± ì™„ë£Œ í›„ ë‹¤ìŒ ì„ íƒì§€ë¥¼ ìœ„í•´ final_documents_to_process ì´ˆê¸°í™”
            final_documents_to_process = []

        print("\n--- RAG ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ---")
                
    except Exception as e:
        print(f"\nğŸš¨ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        pass