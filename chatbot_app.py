# -*- coding: utf-8 -*-
import streamlit as st
from dotenv import load_dotenv
import os
import sys

# RAG components
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain.chains import create_retrieval_chain, LLMChain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.messages import AIMessage, HumanMessage

# Load environment variables from .env file
load_dotenv()

# --- Environment Variables ---
OLLAMA_HOST = os.environ.get('OLLAMA_HOST', 'http://127.0.0.1:11434')
OLLAMA_LLM_MODEL = os.environ.get('OLLAMA_LLM_MODEL', 'llama3')
OLLAMA_EMBEDDING_MODEL = os.environ.get('OLLAMA_EMBEDDING_MODEL', 'nomic-embed-text')
CHROMA_DB_PATH = os.environ.get('CHROMA_DB_PATH', './chroma_db')

# --- RAG Chain Setup (Streamlit ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰) ---
@st.cache_resource
def setup_rag_chain(source_filter=None): # source_filter ì¸ì ì¶”ê°€
    """
    RAG ì²´ì¸ì„ ì„¤ì •í•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    Streamlitì˜ @st.cache_resourceë¥¼ ì‚¬ìš©í•˜ì—¬ ì•±ì´ ë‹¤ì‹œ ë¡œë“œë˜ì–´ë„ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±í•©ë‹ˆë‹¤.
    """
    st.info(f"--- RAG ì²´ì¸ ì„¤ì • ì‹œì‘ (í•„í„°: {source_filter if source_filter else 'ëª¨ë‘'}) ---")
    
    # 1. LLM ë° Embedding Model ë¡œë“œ
    try:
        llm = OllamaLLM(model=OLLAMA_LLM_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
        embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_HOST)
        st.success(f"âœ… Ollama LLM ('{OLLAMA_LLM_MODEL}') ë° Embedding Model ('{OLLAMA_EMBEDDING_MODEL}') ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        st.error(f"âŒ Ollama LLM ë˜ëŠ” Embedding Model ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.warning("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ì§€ì •ëœ ëª¨ë¸ë“¤ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    # 2. Chroma VectorStore ë¡œë“œ
    if not os.path.exists(CHROMA_DB_PATH):
        st.error(f"âŒ Chroma DB ê²½ë¡œ '{CHROMA_DB_PATH}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.warning("ğŸ’¡ 'python ingest.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        st.stop()

    try:
        vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
        
        # --- ë¦¬íŠ¸ë¦¬ë²„ í•„í„°ë§ ì ìš© ---
        search_kwargs = {"k": 5}
        if source_filter:
            search_kwargs["filter"] = {"source_type": source_filter}
            st.info(f"â„¹ï¸ ë¦¬íŠ¸ë¦¬ë²„ í•„í„°ë§ ì ìš©: source_type = '{source_filter}'")
            
        base_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs) 
        
        # --- ContextualCompressionRetriever ì„¤ì • (Custom Prompt ì ìš©) ---
        _compress_template = """ë‹¤ìŒ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ, ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¶€ë¶„ë§Œ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•˜ì„¸ìš”.
        ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, "NO_OUTPUT"ì´ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.
        ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ìŠ¤ë‹ˆí«(ì§§ì€ êµ¬ì ˆ)ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
        
        ë¬¸ì„œ:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ê´€ë ¨ ìŠ¤ë‹ˆí«:"""
        COMPRESSOR_PROMPT = PromptTemplate(template=_compress_template, input_variables=["context", "question"])

        compressor_llm_chain = LLMChain(llm=llm, prompt=COMPRESSOR_PROMPT)
        compressor = LLMChainExtractor(llm_chain=compressor_llm_chain) 
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, 
            base_retriever=base_retriever
        )
        
        st.success(f"âœ… Chroma VectorStore '{CHROMA_DB_PATH}' ë¡œë“œ ë° ContextualCompressionRetriever (k=20, Custom Compressor Prompt) ì„¤ì • ì™„ë£Œ.")
    except Exception as e:
        st.error(f"âŒ Chroma VectorStore ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.stop()

    # 3. RAG í”„ë¡¬í”„íŠ¸ ì •ì˜ (LLMì´ ë‹µë³€ì„ ë” ì˜ ìƒì„±í•˜ë„ë¡ ìœ ë„)
    rag_prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ìœ ëŠ¥í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì„ ì£¼ì˜ ê¹Šê²Œ ì½ê³ , ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì •ë³´ë¥¼ ì°¾ì•„ ìš”ì•½í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    **ë°˜ë“œì‹œ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•˜ë©°, ë‹¹ì‹ ì˜ ì‚¬ì „ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**
    ë§Œì•½ ì£¼ì–´ì§„ ë¬¸ë§¥ì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´,
    **"ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."** ë¼ê³  ëª…í™•í•˜ê²Œ ë§í•˜ì„¸ìš”.
    ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ë¬¸ë§¥ì— ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.

    [ì»¨í…ìŠ¤íŠ¸]
    {context}
    
    [ì§ˆë¬¸]
    {input}
    
    [ë‹µë³€]
    """)

    # 4. RAG ì²´ì¸ êµ¬ì„±
    document_chain = create_stuff_documents_chain(llm, rag_prompt)
    rag_chain = create_retrieval_chain(compression_retriever, document_chain)
    
    st.info("--- RAG ì²´ì¸ êµ¬ì¶• ì™„ë£Œ. ì´ì œ ì§ˆë¬¸í•˜ì„¸ìš”. ---")
    st.info("ğŸ’¡ ì´ ì‹œìŠ¤í…œì€ ë¯¸ë¦¬ ì¸ë±ì‹±ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
    return rag_chain

# --- Streamlit App UI ---
st.set_page_config(page_title="RAG ì±—ë´‡ (ë‹¤ì¤‘ ì†ŒìŠ¤)", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– RAG ì±—ë´‡ (ë‹¤ì¤‘ ì†ŒìŠ¤ ê¸°ë°˜)")
st.caption("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë° ë¡œì»¬ ë¬¸ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

# Sidebar for source selection
with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ")
    selected_source = st.radio(
        "ì–´ë–¤ ì†ŒìŠ¤ì—ì„œ ë‹µë³€ì„ ì°¾ì„ê¹Œìš”?",
        options=["ëª¨ë‘", "ë¸”ë¡œê·¸", "ë¡œì»¬ ë¬¸ì„œ"],
        key="source_selection"
    )
    st.markdown("---")
    st.info("ìƒˆë¡œìš´ ì†ŒìŠ¤ ì„ íƒ ì‹œ, RAG ì²´ì¸ì´ ì¬ì„¤ì •ë©ë‹ˆë‹¤.")

# Map selection to filter value
source_filter_value = None
if selected_source == "ë¸”ë¡œê·¸":
    source_filter_value = "blog"
elif selected_source == "ë¡œì»¬ ë¬¸ì„œ":
    source_filter_value = "local_doc"

# Setup RAG chain based on selected source (runs once for each unique source_filter_value due to @st.cache_resource)
rag_chain = setup_rag_chain(source_filter_value)

# Initialize chat history in session state
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"})

# Display chat messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input from user
if prompt := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            try:
                response = rag_chain.invoke({"input": prompt})
                ai_response = response['answer']
                st.markdown(ai_response)

                if 'context' in response and response['context']:
                    with st.expander("ê²€ìƒ‰ëœ ë¬¸ì„œ (Context) ë¯¸ë¦¬ë³´ê¸° (LLMì— ì „ë‹¬ëœ ìµœì¢… ì»¨í…ìŠ¤íŠ¸)"):
                        for i, doc in enumerate(response['context']):
                            st.write(f"**--- ê²€ìƒ‰ ë¬¸ì„œ {i+1} ({doc.metadata.get('source_type', 'N/A')}) ---**") # source_type í‘œì‹œ
                            st.write(f"**ì œëª©:** {doc.metadata.get('title', 'N/A')}")
                            st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'N/A')}")
                            st.write(f"**ë‚´ìš©:** {doc.page_content}")
                            st.markdown("---")
                else:
                    st.info("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                ai_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(ai_response)
                st.warning("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        st.session_state.messages.append({"role": "assistant", "content": ai_response})